{"revision":{"clientId":"e7f93505-331c-4569-989f-eede9a9989be"},"processorTypes":[{"type":"org.apache.nifi.processors.standard.AttributesToJSON","description":"Generates a JSON representation of the input FlowFile Attributes. The resulting JSON can be written to either a new Attribute 'JSONAttributes' or written to the FlowFile as content.","tags":["flowfile","json","attributes"]},{"type":"org.apache.nifi.processors.standard.Base64EncodeContent","description":"Encodes or decodes content to and from base64","tags":["encode","base64"]},{"type":"org.apache.nifi.processors.standard.CompressContent","description":"Compresses or decompresses the contents of FlowFiles using a user-specified compression algorithm and updates the mime.type attribute as appropriate","tags":["lzma","decompress","compress","snappy framed","gzip","snappy","bzip2","content","xz-lzma2"]},{"type":"org.apache.nifi.amqp.processors.ConsumeAMQP","description":"Consumes AMQP Message transforming its content to a FlowFile and transitioning it to 'success' relationship","tags":["receive","amqp","rabbit","get","consume","message"]},{"type":"org.apache.nifi.jms.processors.ConsumeJMS","description":"Consumes JMS Message of type BytesMessage or TextMessage transforming its content to a FlowFile and transitioning it to 'success' relationship.","tags":["jms","receive","get","consume","message"]},{"type":"org.apache.nifi.processors.standard.ControlRate","description":"Controls the rate at which data is transferred to follow-on processors. If you configure a very small Time Duration, then the accuracy of the throttle gets worse. You can improve this accuracy by decreasing the Yield Duration, at the expense of more Tasks given to the processor.","tags":["throttle","rate","rate control","throughput"]},{"type":"org.apache.nifi.processors.kite.ConvertAvroSchema","description":"Convert records from one Avro schema to another, including support for flattening and simple type conversions","tags":["convert","kite","avro"]},{"type":"org.apache.nifi.processors.avro.ConvertAvroToJSON","description":"Converts a Binary Avro record into a JSON object. This processor provides a direct mapping of an Avro field to a JSON field, such that the resulting JSON will have the same hierarchical structure as the Avro document. Note that the Avro schema information will be lost, as this is not a translation from binary Avro to JSON formatted Avro. The output JSON is encoded the UTF-8 encoding. If an incoming FlowFile contains a stream of multiple Avro records, the resultant FlowFile will contain a JSON Array containing all of the Avro records or a sequence of JSON Objects.  If an incoming FlowFile does not contain any records, an empty JSON object is the output. Empty/Single Avro record FlowFile inputs are optionally wrapped in a container as dictated by 'Wrap Single Record'","tags":["json","convert","avro"]},{"type":"org.apache.nifi.processors.standard.ConvertCharacterSet","description":"Converts a FlowFile's content from one character set to another","tags":["characterset","character set","text","convert"]},{"type":"org.apache.nifi.processors.kite.ConvertCSVToAvro","description":"Converts CSV files to Avro according to an Avro Schema","tags":["csv","kite","avro"]},{"type":"org.apache.nifi.processors.kite.ConvertJSONToAvro","description":"Converts JSON files to Avro according to an Avro Schema","tags":["json","kite","avro"]},{"type":"org.apache.nifi.processors.standard.ConvertJSONToSQL","description":"Converts a JSON-formatted FlowFile into an UPDATE or INSERT SQL statement. The incoming FlowFile is expected to be \"flat\" JSON message, meaning that it consists of a single JSON element and each field maps to a simple type. If a field maps to a JSON object, that JSON object will be interpreted as Text. If the input is an array of JSON elements, each element in the array is output as a separate FlowFile to the 'sql' relationship. Upon successful conversion, the original FlowFile is routed to the 'original' relationship and the SQL is routed to the 'sql' relationship.","tags":["database","rdbms","flat","json","insert","update","relational","sql"]},{"type":"org.apache.nifi.processors.hadoop.CreateHadoopSequenceFile","description":"Creates Hadoop Sequence Files from incoming flow files","tags":["sequencefile","sequence file","create","hadoop"]},{"type":"org.apache.nifi.processors.aws.s3.DeleteS3Object","description":"Deletes FlowFiles on an Amazon S3 Bucket. If attempting to delete a file that does not exist, FlowFile is routed to success.","tags":["S3","Delete","Archive","Amazon","AWS"]},{"type":"org.apache.nifi.processors.aws.sqs.DeleteSQS","description":"Deletes a message from an Amazon Simple Queuing Service Queue","tags":["Delete","SQS","Amazon","AWS","Queue"]},{"type":"org.apache.nifi.processors.standard.DetectDuplicate","description":"Caches a value, computed from FlowFile attributes, for each incoming FlowFile and determines if the cached value has already been seen. If so, routes the FlowFile to 'duplicate' with an attribute named 'original.identifier' that specifies the original FlowFile's \"description\", which is specified in the <FlowFile Description> property. If the FlowFile is not determined to be a duplicate, the Processor routes the FlowFile to 'non-duplicate'","tags":["dedupe","dupe","duplicate","hash"]},{"type":"org.apache.nifi.processors.standard.DistributeLoad","description":"Distributes FlowFiles to downstream processors based on a Distribution Strategy. If using the Round Robin strategy, the default is to assign each destination a weighting of 1 (evenly distributed). However, optional propertiescan be added to the change this; adding a property with the name '5' and value '10' means that the relationship with name '5' will be receive 10 FlowFiles in each iteration instead of 1.","tags":["route","round robin","load balance","distribute","weighted"]},{"type":"org.apache.nifi.processors.standard.DuplicateFlowFile","description":"Intended for load testing, this processor will create the configured number of copies of each incoming FlowFile","tags":["test","load","duplicate"]},{"type":"org.apache.nifi.processors.standard.EncryptContent","description":"Encrypts or Decrypts a FlowFile using either symmetric encryption with a password and randomly generated salt, or asymmetric encryption using a public and secret key.","tags":["password","OpenPGP","encryption","PGP","JCE","decryption","GPG"]},{"type":"org.apache.nifi.processors.standard.EvaluateJsonPath","description":"Evaluates one or more JsonPath expressions against the content of a FlowFile. The results of those expressions are assigned to FlowFile Attributes or are written to the content of the FlowFile itself, depending on configuration of the Processor. JsonPaths are entered by adding user-defined properties; the name of the property maps to the Attribute Name into which the result will be placed (if the Destination is flowfile-attribute; otherwise, the property name is ignored). The value of the property must be a valid JsonPath expression. A Return Type of 'auto-detect' will make a determination based off the configured destination. When 'Destination' is set to 'flowfile-attribute,' a return type of 'scalar' will be used. When 'Destination' is set to 'flowfile-content,' a return type of 'JSON' will be used.If the JsonPath evaluates to a JSON array or JSON object and the Return Type is set to 'scalar' the FlowFile will be unmodified and will be routed to failure. A Return Type of JSON can return scalar values if the provided JsonPath evaluates to the specified value and will be routed as a match.If Destination is 'flowfile-content' and the JsonPath does not evaluate to a defined path, the FlowFile will be routed to 'unmatched' without having its contents modified. If Destination is flowfile-attribute and the expression matches nothing, attributes will be created with empty strings as the value, and the FlowFile will always be routed to 'matched.'","tags":["JSON","JsonPath","evaluate"]},{"type":"org.apache.nifi.processors.standard.EvaluateRegularExpression","description":"WARNING: This has been deprecated and will be removed in 0.2.0.  \n\n Use ExtractText instead.","tags":["deprecated"]},{"type":"org.apache.nifi.processors.standard.EvaluateXPath","description":"Evaluates one or more XPaths against the content of a FlowFile. The results of those XPaths are assigned to FlowFile Attributes or are written to the content of the FlowFile itself, depending on configuration of the Processor. XPaths are entered by adding user-defined properties; the name of the property maps to the Attribute Name into which the result will be placed (if the Destination is flowfile-attribute; otherwise, the property name is ignored). The value of the property must be a valid XPath expression. If the XPath evaluates to more than one node and the Return Type is set to 'nodeset' (either directly, or via 'auto-detect' with a Destination of 'flowfile-content'), the FlowFile will be unmodified and will be routed to failure. If the XPath does not evaluate to a Node, the FlowFile will be routed to 'unmatched' without having its contents modified. If Destination is flowfile-attribute and the expression matches nothing, attributes will be created with empty strings as the value, and the FlowFile will always be routed to 'matched'","tags":["XPath","XML","evaluate"]},{"type":"org.apache.nifi.processors.standard.EvaluateXQuery","description":"Evaluates one or more XQueries against the content of a FlowFile.  The results of those XQueries are assigned to FlowFile Attributes or are written to the content of the FlowFile itself, depending on configuration of the Processor.  XQueries are entered by adding user-defined properties; the name of the property maps to the Attribute Name into which the result will be placed (if the Destination is 'flowfile-attribute'; otherwise, the property name is ignored).  The value of the property must be a valid XQuery.  If the XQuery returns more than one result, new attributes or FlowFiles (for Destinations of 'flowfile-attribute' or 'flowfile-content' respectively) will be created for each result (attributes will have a '.n' one-up number appended to the specified attribute name).  If any provided XQuery returns a result, the FlowFile(s) will be routed to 'matched'. If no provided XQuery returns a result, the FlowFile will be routed to 'unmatched'.  If the Destination is 'flowfile-attribute' and the XQueries matche nothing, no attributes will be applied to the FlowFile.","tags":["XPath","XML","XQuery","evaluate"]},{"type":"org.apache.nifi.processors.flume.ExecuteFlumeSink","description":"Execute a Flume sink. Each input FlowFile is converted into a Flume Event for processing by the sink.","tags":["sink","flume","hadoop","put"]},{"type":"org.apache.nifi.processors.flume.ExecuteFlumeSource","description":"Execute a Flume source. Each Flume Event is sent to the success relationship as a FlowFile","tags":["flume","get","hadoop","source"]},{"type":"org.apache.nifi.processors.standard.ExecuteProcess","description":"Runs an operating system command specified by the user and writes the output of that command to a FlowFile. If the command is expected to be long-running, the Processor can output the partial data on a specified interval. When this option is used, the output is expected to be in textual format, as it typically does not make sense to split binary data on arbitrary time-based intervals.","tags":["process","external","invoke","source","command","script"]},{"type":"org.apache.nifi.processors.script.ExecuteScript","description":"Experimental - Executes a script given the flow file and a process session.  The script is responsible for handling the incoming flow file (transfer to SUCCESS or remove, e.g.) as well as any flow files created by the script. If the handling is incomplete or incorrect, the session will be rolled back. Experimental: Impact of sustained usage not yet verified.","tags":["luaj","python","groovy","jython","js","lua","execute","script","javascript","jruby","ruby"]},{"type":"org.apache.nifi.processors.standard.ExecuteSQL","description":"Execute provided SQL select query. Query result will be converted to Avro format. Streaming is used so arbitrarily large result sets are supported. This processor can be scheduled to run on a timer, or cron expression, using the standard scheduling methods, or it can be triggered by an incoming FlowFile. If it is triggered by an incoming FlowFile, then attributes of that FlowFile will be available when evaluating the select query. FlowFile attribute 'executesql.row.count' indicates how many rows were selected.","tags":["database","select","query","jdbc","sql"]},{"type":"org.apache.nifi.processors.standard.ExecuteStreamCommand","description":"Executes an external command on the contents of a flow file, and creates a new flow file with the results of the command.","tags":["command execution","stream","execute","command"]},{"type":"org.apache.nifi.processors.avro.ExtractAvroMetadata","description":"Extracts metadata from the header of an Avro datafile.","tags":["schema","metadata","avro"]},{"type":"org.apache.nifi.processors.hl7.ExtractHL7Attributes","description":"Extracts information from an HL7 (Health Level 7) formatted FlowFile and adds the information as FlowFile Attributes. The attributes are named as <Segment Name> <dot> <Field Index>. If the segment is repeating, the naming will be <Segment Name> <underscore> <Segment Index> <dot> <Field Index>. For example, we may have an attribute named \"MHS.12\" with a value of \"2.1\" and an attribute named \"OBX_11.3\" with a value of \"93000^CPT4\".","tags":["HL7","extract","attributes","health level 7","healthcare"]},{"type":"org.apache.nifi.processors.image.ExtractImageMetadata","description":"Extract the image metadata from flowfiles containing images. This processor relies on this metadata extractor library https://github.com/drewnoakes/metadata-extractor. It extracts a long list of metadata types including but not limited to EXIF, IPTC, XMP and Photoshop fields. For the full list visit the library's website.NOTE: The library being used loads the images into memory so extremely large images may cause problems.","tags":["JPG","Exif","image","metadata","file","BMP","IPTC","Exchangeable","GIF","format","PNG","XMP"]},{"type":"org.apache.nifi.processors.standard.ExtractText","description":"Evaluates one or more Regular Expressions against the content of a FlowFile.  The results of those Regular Expressions are assigned to FlowFile Attributes.  Regular Expressions are entered by adding user-defined properties; the name of the property maps to the Attribute Name into which the result will be placed.  The first capture group, if any found, will be placed into that attribute name.But all capture groups, including the matching string sequence itself will also be provided at that attribute name with an index value provided, with the exception of a capturing group that is optional and does not match - for example, given the attribute name \"regex\" and expression \"abc(def)?(g)\" we would add an attribute \"regex.1\" with a value of \"def\" if the \"def\" matched. If the \"def\" did not match, no attribute named \"regex.1\" would be added but an attribute named \"regex.2\" with a value of \"g\" will be added regardless.The value of the property must be a valid Regular Expressions with one or more capturing groups.  If the Regular Expression matches more than once, only the first match will be used.  If any provided Regular Expression matches, the FlowFile(s) will be routed to 'matched'. If no provided Regular Expression matches, the FlowFile will be routed to 'unmatched' and no attributes will be applied to the FlowFile.","tags":["Regular Expression","regex","extract","Text","evaluate"]},{"type":"org.apache.nifi.processors.standard.FetchDistributedMapCache","description":"Computes a cache key from FlowFile attributes, for each incoming FlowFile, and fetches the value from the Distributed Map Cache associated with that key. The incoming FlowFile's content is replaced with the binary data received by the Distributed Map Cache. If there is no value stored under that key then the flow file will be routed to 'not-found'. Note that the processor will always attempt to read the entire cached value into memory before placing it in it's destination. This could be potentially problematic if the cached value is very large.","tags":["cache","fetch","distributed","map"]},{"type":"org.apache.nifi.processors.elasticsearch.FetchElasticsearch","description":"Retrieves a document from Elasticsearch using the specified connection properties and the identifier of the document to retrieve. If the cluster has been configured for authorization and/or secure transport (SSL/TLS) and the Shield plugin is available, secure connections can be made. This processor supports Elasticsearch 2.x clusters.","tags":["read","elasticsearch","fetch","get"]},{"type":"org.apache.nifi.processors.standard.FetchFile","description":"Reads the contents of a file from disk and streams it into the contents of an incoming FlowFile. Once this is done, the file is optionally moved elsewhere or deleted to help keep the file system organized.","tags":["ingress","input","get","files","source","local","filesystem","ingest"]},{"type":"org.apache.nifi.processors.hadoop.FetchHDFS","description":"Retrieves a file from HDFS. The content of the incoming FlowFile is replaced by the content of the file in HDFS. The file in HDFS is left intact without any changes being made to it.","tags":["get","fetch","hdfs","hadoop","source","ingest"]},{"type":"org.apache.nifi.processors.aws.s3.FetchS3Object","description":"Retrieves the contents of an S3 Object and writes it to the content of a FlowFile","tags":["S3","Fetch","Get","Amazon","AWS"]},{"type":"org.apache.nifi.processors.standard.FetchSFTP","description":"Fetches the content of a file from a remote SFTP server and overwrites the contents of an incoming FlowFile with the content of the remote file.","tags":["input","get","fetch","retrieve","files","sftp","source","remote","ingest"]},{"type":"org.apache.nifi.processors.standard.GenerateFlowFile","description":"This processor creates FlowFiles of random data and is used for load testing","tags":["random","test","generate"]},{"type":"org.apache.nifi.processors.GeoEnrichIP","description":"Looks up geolocation information for an IP address and adds the geo information to FlowFile attributes. The geo data is provided as a MaxMind database. The attribute that contains the IP address to lookup is provided by the 'IP Address Attribute' property. If the name of the attribute provided is 'X', then the the attributes added by enrichment will take the form X.geo.<fieldName>","tags":["geo","ip","enrich","maxmind"]},{"type":"org.apache.nifi.processors.azure.eventhub.GetAzureEventHub","description":"Receives messages from a Microsoft Azure Event Hub, writing the contents of the Azure message to the content of the FlowFile","tags":["cloud","streaming","streams","eventhub","microsoft","events","azure"]},{"type":"org.apache.nifi.processors.couchbase.GetCouchbaseKey","description":"Get a document from Couchbase Server via Key/Value access. The ID of the document to fetch may be supplied by setting the <Document Id> property. NOTE: if the Document Id property is not set, the contents of the FlowFile will be read to determine the Document Id, which means that the contents of the entire FlowFile will be buffered in memory.","tags":["database","couchbase","get","nosql"]},{"type":"org.apache.nifi.processors.standard.GetFile","description":"Creates FlowFiles from files in a directory.  NiFi will ignore files it doesn't have at least read permissions for.","tags":["ingress","input","get","files","source","local","filesystem","ingest"]},{"type":"org.apache.nifi.processors.standard.GetFTP","description":"Fetches files from an FTP Server and creates FlowFiles from them","tags":["input","FTP","get","fetch","retrieve","files","source","remote","ingest"]},{"type":"org.apache.nifi.hbase.GetHBase","description":"This Processor polls HBase for any records in the specified table. The processor keeps track of the timestamp of the cells that it receives, so that as new records are pushed to HBase, they will automatically be pulled. Each record is output in JSON format, as {\"row\": \"<row key>\", \"cells\": { \"<column 1 family>:<column 1 qualifier>\": \"<cell 1 value>\", \"<column 2 family>:<column 2 qualifier>\": \"<cell 2 value>\", ... }}. For each record received, a Provenance RECEIVE event is emitted with the format hbase://<table name>/<row key>, where <row key> is the UTF-8 encoded value of the row's key.","tags":["get","hbase","ingest"]},{"type":"org.apache.nifi.processors.hadoop.GetHDFS","description":"Fetch files from Hadoop Distributed File System (HDFS) into FlowFiles. This Processor will delete the file from HDFS after fetching it.","tags":["get","fetch","HDFS","hadoop","source","filesystem","ingest"]},{"type":"org.apache.nifi.processors.hadoop.GetHDFSSequenceFile","description":"Fetch sequence files from Hadoop Distributed File System (HDFS) into FlowFiles","tags":["get","fetch","sequence file","HDFS","hadoop","source","ingest"]},{"type":"org.apache.nifi.GetHTMLElement","description":"Extracts HTML element values from the incoming flowfile's content using a CSS selector. The incoming HTML is first converted into a HTML Document Object Model so that HTML elements may be selected in the similar manner that CSS selectors are used to apply styles to HTML. The resulting HTML DOM is then \"queried\" using the user defined CSS selector string. The result of \"querying\" the HTML DOM may produce 0-N results. If no results are found the flowfile will be transferred to the \"element not found\" relationship to indicate so to the end user. If N results are found a new flowfile will be created and emitted for each result. The query result will either be placed in the content of the new flowfile or as an attribute of the new flowfile. By default the result is written to an attribute. This can be controlled by the \"Destination\" property. Resulting query values may also have data prepended or appended to them by setting the value of property \"Prepend Element Value\" or \"Append Element Value\". Prepended and appended values are treated as string values and concatenated to the result retrieved from the HTML DOM query operation. A more thorough reference for the CSS selector syntax can be found at \"http://jsoup.org/apidocs/org/jsoup/select/Selector.html\"","tags":["css","dom","get","html","element"]},{"type":"org.apache.nifi.processors.standard.GetHTTP","description":"Fetches data from an HTTP or HTTPS URL and writes the data to the content of a FlowFile. Once the content has been fetched, the ETag and Last Modified dates are remembered (if the web server supports these concepts). This allows the Processor to fetch new data only if the remote data has changed or until the state is cleared. That is, once the content has been fetched from the given URL, it will not be fetched again until the content on the remote server changes. Note that due to limitations on state management, stored \"last modified\" and etag fields never expire. If the URL in GetHttp uses Expression Language that is unbounded, there is the potential for Out of Memory Errors to occur.","tags":["input","get","fetch","http","poll","https","source","ingest"]},{"type":"org.apache.nifi.processors.standard.GetJMSQueue","description":"Pulls messages from a JMS Queue, creating a FlowFile for each JMS Message or bundle of messages, as configured","tags":["jms","pull","get","consume","source","queue","listen","consumer"]},{"type":"org.apache.nifi.processors.standard.GetJMSTopic","description":"Pulls messages from a JMS Topic, creating a FlowFile for each JMS Message or bundle of messages, as configured","tags":["jms","durable","pull","get","non-durable","topic","consume","subscription","source","listen","consumer"]},{"type":"org.apache.nifi.processors.kafka.GetKafka","description":"Fetches messages from Apache Kafka","tags":["PubSub","Ingest","Get","Kafka","Ingress","Apache","Topic"]},{"type":"org.apache.nifi.processors.mongodb.GetMongo","description":"Creates FlowFiles from documents in MongoDB","tags":["read","get","mongodb"]},{"type":"org.apache.nifi.processors.standard.GetSFTP","description":"Fetches files from an SFTP Server and creates FlowFiles from them","tags":["input","get","fetch","retrieve","files","sftp","source","remote","ingest"]},{"type":"org.apache.nifi.processors.solr.GetSolr","description":"Queries Solr and outputs the results as a FlowFile","tags":["Pull","Solr","Get","Apache"]},{"type":"org.apache.nifi.processors.splunk.GetSplunk","description":"Retrieves data from Splunk Enterprise.","tags":["get","splunk","logs"]},{"type":"org.apache.nifi.processors.aws.sqs.GetSQS","description":"Fetches messages from an Amazon Simple Queuing Service Queue","tags":["Fetch","SQS","Get","Poll","Amazon","AWS","Queue"]},{"type":"org.apache.nifi.processors.twitter.GetTwitter","description":"Pulls status changes from Twitter's streaming API","tags":["twitter","json","tweets","social media","status"]},{"type":"org.apache.nifi.processors.standard.HandleHttpRequest","description":"Starts an HTTP Server and listens for HTTP Requests. For each request, creates a FlowFile and transfers to 'success'. This Processor is designed to be used in conjunction with the HandleHttpResponse Processor in order to create a Web Service","tags":["request","ingress","http","https","web service","listen"]},{"type":"org.apache.nifi.processors.standard.HandleHttpResponse","description":"Sends an HTTP Response to the Requestor that generated a FlowFile. This Processor is designed to be used in conjunction with the HandleHttpRequest in order to create a web service.","tags":["response","http","https","web service","egress"]},{"type":"org.apache.nifi.processors.standard.HashAttribute","description":"Hashes together the key/value pairs of several FlowFile Attributes and adds the hash as a new attribute. Optional properties are to be added such that the name of the property is the name of a FlowFile Attribute to consider and the value of the property is a regular expression that, if matched by the attribute value, will cause that attribute to be used as part of the hash. If the regular expression contains a capturing group, only the value of the capturing group will be used.","tags":["attributes","hash"]},{"type":"org.apache.nifi.processors.standard.HashContent","description":"Calculates a hash value for the Content of a FlowFile and puts that hash value on the FlowFile as an attribute whose name is determined by the <Hash Attribute Name> property","tags":["SHA-1","SHA-256","hash","content","MD5"]},{"type":"org.apache.nifi.processors.standard.IdentifyMimeType","description":"Attempts to identify the MIME Type used for a FlowFile. If the MIME Type can be identified, an attribute with the name 'mime.type' is added with the value being the MIME Type. If the MIME Type cannot be determined, the value will be set to 'application/octet-stream'. In addition, the attribute mime.extension will be set if a common file extension for the MIME Type is known.","tags":["zip","MIME","file","identify","gzip","mime.type","compression","bzip2"]},{"type":"org.apache.nifi.processors.kite.InferAvroSchema","description":"Examines the contents of the incoming FlowFile to infer an Avro schema. The processor will use the Kite SDK to make an attempt to automatically generate an Avro schema from the incoming content. When inferring the schema from JSON data the key names will be used in the resulting Avro schema definition. When inferring from CSV data a \"header definition\" must be present either as the first line of the incoming data or the \"header definition\" must be explicitly set in the property \"CSV Header Definition\". A \"header definition\" is simply a single comma separated line defining the names of each column. The \"header definition\" is required in order to determine the names that should be given to each field in the resulting Avro definition. When inferring data types the higher order data type is always used if there is ambiguity. For example when examining numerical values the type may be set to \"long\" instead of \"integer\" since a long can safely hold the value of any \"integer\". Only CSV and JSON content is currently supported for automatically inferring an Avro schema. The type of content present in the incoming FlowFile is set by using the property \"Input Content Type\". The property can either be explicitly set to CSV, JSON, or \"use mime.type value\" which will examine the value of the mime.type attribute on the incoming FlowFile to determine the type of content present.","tags":["schema","infer","csv","json","kite","avro"]},{"type":"org.apache.nifi.processors.standard.InvokeHTTP","description":"An HTTP client processor which can interact with a configurable HTTP Endpoint. The destination URL and HTTP Method are configurable. FlowFile attributes are converted to HTTP headers and the FlowFile contents are included as the body of the request (if the HTTP Method is PUT or POST).","tags":["rest","http","client","https"]},{"type":"org.apache.nifi.processors.script.InvokeScriptedProcessor","description":"Experimental - Invokes a script engine for a Processor defined in the given script. The script must define a valid class that implements the Processor interface, and it must set a variable 'processor' to an instance of the class. Processor methods such as onTrigger() will be delegated to the scripted Processor instance. Also any Relationships or PropertyDescriptors defined by the scripted processor will be added to the configuration dialog.  Experimental: Impact of sustained usage not yet verified.","tags":["luaj","python","groovy","jython","js","lua","invoke","script","javascript","jruby","ruby"]},{"type":"org.apache.nifi.processors.standard.ListenHTTP","description":"Starts an HTTP Server that is used to receive FlowFiles from remote sources. The default URI of the Service will be http://{hostname}:{port}/contentListener","tags":["rest","http","https","listen","ingest"]},{"type":"org.apache.nifi.processors.standard.ListenRELP","description":"Listens for RELP messages being sent to a given port over TCP. Each message will be acknowledged after successfully writing the message to a FlowFile. Each FlowFile will contain data portion of one or more RELP frames. In the case where the RELP frames contain syslog messages, the output of this processor can be sent to a ParseSyslog processor for further processing.","tags":["tcp","relp","logs","listen"]},{"type":"org.apache.nifi.processors.standard.ListenSyslog","description":"Listens for Syslog messages being sent to a given port over TCP or UDP. Incoming messages are checked against regular expressions for RFC5424 and RFC3164 formatted messages. The format of each message is: (<PRIORITY>)(VERSION )(TIMESTAMP) (HOSTNAME) (BODY) where version is optional. The timestamp can be an RFC5424 timestamp with a format of \"yyyy-MM-dd'T'HH:mm:ss.SZ\" or \"yyyy-MM-dd'T'HH:mm:ss.S+hh:mm\", or it can be an RFC3164 timestamp with a format of \"MMM d HH:mm:ss\". If an incoming messages matches one of these patterns, the message will be parsed and the individual pieces will be placed in FlowFile attributes, with the original message in the content of the FlowFile. If an incoming message does not match one of these patterns it will not be parsed and the syslog.valid attribute will be set to false with the original message in the content of the FlowFile. Valid messages will be transferred on the success relationship, and invalid messages will be transferred on the invalid relationship.","tags":["udp","tcp","syslog","logs","listen"]},{"type":"org.apache.nifi.processors.standard.ListenTCP","description":"Listens for incoming TCP connections and reads data from each connection using a line separator as the message demarcator. The default behavior is for each message to produce a single FlowFile, however this can be controlled by increasing the Batch Size to a larger value for higher throughput. The Receive Buffer Size must be set as large as the largest messages expected to be received, meaning if every 100kb there is a line separator, then the Receive Buffer Size must be greater than 100kb.","tags":["tcp","tls","ssl","listen"]},{"type":"org.apache.nifi.processors.standard.ListenUDP","description":"Listens for Datagram Packets on a given port. The default behavior produces a FlowFile per datagram, however for higher throughput the Max Batch Size property may be increased to specify the number of datagrams to batch together in a single FlowFile. This processor can be restricted to listening for datagrams from  a specific remote host and port by specifying the Sending Host and Sending Host Port properties, otherwise it will listen for datagrams from all hosts and ports.","tags":["udp","source","listen","ingest"]},{"type":"org.apache.nifi.processors.standard.ListFile","description":"Retrieves a listing of files from the local filesystem. For each file that is listed, creates a FlowFile that represents the file so that it can be fetched in conjunction with ListFile. This Processor is designed to run on Primary Node only in a cluster. If the primary node changes, the new Primary Node will pick up where the previous node left off without duplicating all of the data. Unlike GetFile, this Processor does not delete any data from the local filesystem.","tags":["file","get","source","list","filesystem","ingest"]},{"type":"org.apache.nifi.processors.hadoop.ListHDFS","description":"Retrieves a listing of files from HDFS. For each file that is listed in HDFS, creates a FlowFile that represents the HDFS file so that it can be fetched in conjunction with ListHDFS. This Processor is designed to run on Primary Node only in a cluster. If the primary node changes, the new Primary Node will pick up where the previous node left off without duplicating all of the data. Unlike GetHDFS, this Processor does not delete any data from HDFS.","tags":["get","HDFS","hadoop","source","list","filesystem","ingest"]},{"type":"org.apache.nifi.processors.standard.ListSFTP","description":"Performs a listing of the files residing on an SFTP server. For each file that is found on the remote server, a new FlowFile will be created with the filename attribute set to the name of the file on the remote server. This can then be used in conjunction with FetchSFTP in order to fetch those files.","tags":["input","files","sftp","source","list","remote","ingest"]},{"type":"org.apache.nifi.processors.standard.LogAttribute","tags":["logging","attributes"]},{"type":"org.apache.nifi.processors.standard.MergeContent","description":"Merges a Group of FlowFiles together based on a user-defined strategy and packages them into a single FlowFile. It is recommended that the Processor be configured with only a single incoming connection, as Group of FlowFiles will not be created from FlowFiles in different connections. This processor updates the mime.type attribute as appropriate.","tags":["zip","flowfile-stream-v3","flowfile-stream","concatenation","correlation","tar","stream","merge","archive","content"]},{"type":"org.apache.nifi.processors.standard.ModifyBytes","description":"Keep or discard bytes range from a binary file.","tags":["discard","binary","keep"]},{"type":"org.apache.nifi.ModifyHTMLElement","description":"Modifies the value of an existing HTML element. The desired element to be modified is located by using CSS selector syntax. The incoming HTML is first converted into a HTML Document Object Model so that HTML elements may be selected in the similar manner that CSS selectors are used to apply styles to HTML. The resulting HTML DOM is then \"queried\" using the user defined CSS selector string to find the element the user desires to modify. If the HTML element is found the element's value is updated in the DOM using the value specified \"Modified Value\" property. All DOM elements that match the CSS selector will be updated. Once all of the DOM elements have been updated the DOM is rendered to HTML and the result replaces the flowfile content with the updated HTML. A more thorough reference for the CSS selector syntax can be found at \"http://jsoup.org/apidocs/org/jsoup/select/Selector.html\"","tags":["modify","css","dom","html","element"]},{"type":"org.apache.nifi.processors.standard.MonitorActivity","description":"Monitors the flow for activity and sends out an indicator when the flow has not had any data for some specified amount of time and again when the flow's activity is restored","tags":["detection","inactive","activity","active","monitor","flow"]},{"type":"org.apache.nifi.processors.standard.ParseSyslog","description":"Parses the contents of a Syslog message and adds attributes to the FlowFile for each of the parts of the Syslog message","tags":["system","attributes","syslog","event","message","logs"]},{"type":"org.apache.nifi.processors.standard.PostHTTP","description":"Performs an HTTP Post with the content of the FlowFile","tags":["http","archive","https","copy","remote"]},{"type":"org.apache.nifi.amqp.processors.PublishAMQP","description":"Creates a AMQP Message from the contents of a FlowFile and sends the message to an AMQP Exchange.In a typical AMQP exchange model, the message that is sent to the AMQP Exchange will be routed based on the 'Routing Key' to its final destination in the queue (the binding). If due to some misconfiguration the binding between the Exchange, Routing Key and Queue is not set up, the message will have no final destination and will return (i.e., the data will not make it to the queue). If that happens you will see a log in both app-log and bulletin stating to that effect. Fixing the binding (normally done by AMQP administrator) will resolve the issue.","tags":["amqp","rabbit","publish","message","send","put"]},{"type":"org.apache.nifi.jms.processors.PublishJMS","description":"Creates a JMS Message from the contents of a FlowFile and sends it to a JMS Destination (queue or topic) as JMS BytesMessage.","tags":["jms","publish","message","send","put"]},{"type":"org.apache.nifi.processors.azure.eventhub.PutAzureEventHub","description":"Sends the contents of a FlowFile to a Windows Azure Event Hub. Note: the content of the FlowFile will be buffered into memory before being sent, so care should be taken to avoid sending FlowFiles to this Processor that exceed the amount of Java Heap Space available.","tags":["cloud","streaming","streams","eventhub","microsoft","events","azure"]},{"type":"org.apache.nifi.processors.cassandra.PutCassandraQL","description":"Execute provided Cassandra Query Language (CQL) statement on a Cassandra 1.x or 2.x cluster. The content of an incoming FlowFile is expected to be the CQL command to execute. The CQL command may use the ? to escape parameters. In this case, the parameters to use must exist as FlowFile attributes with the naming convention cql.args.N.type and cql.args.N.value, where N is a positive integer. The cql.args.N.type is expected to be a lowercase string indicating the Cassandra type.","tags":["set","cassandra","insert","update","put","cql"]},{"type":"org.apache.nifi.processors.couchbase.PutCouchbaseKey","description":"Put a document to Couchbase Server via Key/Value access.","tags":["database","couchbase","put","nosql"]},{"type":"org.apache.nifi.processors.standard.PutDistributedMapCache","description":"Gets the content of a FlowFile and puts it to a distributed map cache, using a cache key computed from FlowFile attributes. If the cache already contains the entry and the cache update strategy is 'keep original' the entry is not replaced.'","tags":["cache","distributed","map","put"]},{"type":"org.apache.nifi.processors.elasticsearch.PutElasticsearch","description":"Writes the contents of a FlowFile to Elasticsearch, using the specified parameters such as the index to insert into and the type of the document. If the cluster has been configured for authorization and/or secure transport (SSL/TLS) and the Shield plugin is available, secure connections can be made. This processor supports Elasticsearch 2.x clusters.","tags":["elasticsearch","insert","update","write","put"]},{"type":"org.apache.nifi.processors.standard.PutEmail","description":"Sends an e-mail to configured recipients for each incoming FlowFile","tags":["smtp","email","put","notify"]},{"type":"org.apache.nifi.processors.standard.PutFile","description":"Writes the contents of a FlowFile to the local file system","tags":["files","archive","copy","put","local","filesystem"]},{"type":"org.apache.nifi.processors.standard.PutFTP","description":"Sends FlowFiles to an FTP Server","tags":["ftp","files","archive","copy","remote","put","egress"]},{"type":"org.apache.nifi.hbase.PutHBaseCell","description":"Adds the Contents of a FlowFile to HBase as the value of a single cell","tags":["hadoop","hbase"]},{"type":"org.apache.nifi.hbase.PutHBaseJSON","description":"Adds rows to HBase based on the contents of incoming JSON documents. Each FlowFile must contain a single UTF-8 encoded JSON document, and any FlowFiles where the root element is not a single document will be routed to failure. Each JSON field name and value will become a column qualifier and value of the HBase row. Any fields with a null value will be skipped, and fields with a complex value will be handled according to the Complex Field Strategy. The row id can be specified either directly on the processor through the Row Identifier property, or can be extracted from the JSON document by specifying the Row Identifier Field Name property. This processor will hold the contents of all FlowFiles for the given batch in memory at one time.","tags":["json","hadoop","hbase","put"]},{"type":"org.apache.nifi.processors.hadoop.PutHDFS","description":"Write FlowFile data to Hadoop Distributed File System (HDFS)","tags":["HDFS","hadoop","copy","put","filesystem"]},{"type":"org.apache.nifi.PutHTMLElement","description":"Places a new HTML element in the existing HTML DOM. The desired position for the new HTML element is specified by using CSS selector syntax. The incoming HTML is first converted into a HTML Document Object Model so that HTML DOM location may be located in a similar manner that CSS selectors are used to apply styles to HTML. The resulting HTML DOM is then \"queried\" using the user defined CSS selector string to find the position where the user desires to add the new HTML element. Once the new HTML element is added to the DOM it is rendered to HTML and the result replaces the flowfile content with the updated HTML. A more thorough reference for the CSS selector syntax can be found at \"http://jsoup.org/apidocs/org/jsoup/select/Selector.html\"","tags":["css","dom","html","put","element"]},{"type":"org.apache.nifi.processors.standard.PutJMS","description":"Creates a JMS Message from the contents of a FlowFile and sends the message to a JMS Server","tags":["jms","send","put"]},{"type":"org.apache.nifi.processors.kafka.PutKafka","description":"Sends the contents of a FlowFile as a message to Apache Kafka. The messages to send may be individual FlowFiles or may be delimited, using a user-specified delimiter, such as a new-line.","tags":["PubSub","Message","Kafka","Apache","Put","Send"]},{"type":"org.apache.nifi.processors.aws.kinesis.firehose.PutKinesisFirehose","description":"Sends the contents to a specified Amazon Kinesis Firehose. In order to send data to firehose, the firehose delivery stream name has to be specified.","tags":["amazon","firehose","stream","aws","kinesis","put"]},{"type":"org.apache.nifi.processors.aws.lambda.PutLambda","description":"Sends the contents to a specified Amazon Lamba Function. The AWS credentials used for authentication must have permissions execute the Lambda function (lambda:InvokeFunction).The FlowFile content must be JSON.","tags":["amazon","lambda","aws","put"]},{"type":"org.apache.nifi.processors.mongodb.PutMongo","description":"Writes the contents of a FlowFile to MongoDB","tags":["insert","update","mongodb","write","put"]},{"type":"org.apache.nifi.processors.riemann.PutRiemann","description":"Send events to Riemann (http://riemann.io) when FlowFiles pass through this processor. You can use events to notify Riemann that a FlowFile passed through, or you can attach a more meaningful metric, such as, the time a FlowFile took to get to this processor. All attributes attached to events support the NiFi Expression Language.","tags":["riemann","metrics","monitoring"]},{"type":"org.apache.nifi.processors.aws.s3.PutS3Object","description":"Puts FlowFiles to an Amazon S3 Bucket\nThe upload uses either the PutS3Object method or PutS3MultipartUpload methods.  The PutS3Object method send the file in a single synchronous call, but it has a 5GB size limit.  Larger files are sent using the multipart upload methods that initiate, transfer the parts, and complete an upload.  This multipart process saves state after each step so that a large upload can be resumed with minimal loss if the processor or cluster is stopped and restarted.\nA multipart upload consists of three steps\n  1) initiate upload,\n  2) upload the parts, and\n  3) complete the upload.\nFor multipart uploads, the processor saves state locally tracking the upload ID and parts uploaded, which must both be provided to complete the upload.\nThe AWS libraries select an endpoint URL based on the AWS region, but this can be overridden with the 'Endpoint Override URL' property for use with other S3-compatible endpoints.\nThe S3 API specifies that the maximum file size for a PutS3Object upload is 5GB. It also requires that parts in a multipart upload must be at least 5MB in size, except for the last part.  These limits are establish the bounds for the Multipart Upload Threshold and Part Size properties.","tags":["S3","Archive","Amazon","AWS","Put"]},{"type":"org.apache.nifi.processors.standard.PutSFTP","description":"Sends FlowFiles to an SFTP Server","tags":["files","sftp","archive","copy","remote","put","egress"]},{"type":"org.apache.nifi.processors.aws.sns.PutSNS","description":"Sends the content of a FlowFile as a notification to the Amazon Simple Notification Service","tags":["amazon","publish","sns","topic","aws","put","pubsub"]},{"type":"org.apache.nifi.processors.solr.PutSolrContentStream","description":"Sends the contents of a FlowFile as a ContentStream to Solr","tags":["Solr","Apache","Put","Send"]},{"type":"org.apache.nifi.processors.splunk.PutSplunk","description":"Sends logs to Splunk Enterprise over TCP, TCP + TLS/SSL, or UDP. If a Message Delimiter is provided, then this processor will read messages from the incoming FlowFile based on the delimiter, and send each message to Splunk. If a Message Delimiter is not provided then the content of the FlowFile will be sent directly to Splunk as if it were a single message.","tags":["tcp","udp","splunk","logs"]},{"type":"org.apache.nifi.processors.standard.PutSQL","description":"Executes a SQL UPDATE or INSERT command. The content of an incoming FlowFile is expected to be the SQL command to execute. The SQL command may use the ? to escape parameters. In this case, the parameters to use must exist as FlowFile attributes with the naming convention sql.args.N.type and sql.args.N.value, where N is a positive integer. The sql.args.N.type is expected to be a number indicating the JDBC Type. The content of the FlowFile is expected to be in UTF-8 format.","tags":["database","rdbms","update","insert","relational","put","sql"]},{"type":"org.apache.nifi.processors.aws.sqs.PutSQS","description":"Publishes a message to an Amazon Simple Queuing Service Queue","tags":["SQS","Amazon","AWS","Queue","Put","Publish"]},{"type":"org.apache.nifi.processors.standard.PutSyslog","description":"Sends Syslog messages to a given host and port over TCP or UDP. Messages are constructed from the \"Message ___\" properties of the processor which can use expression language to generate messages from incoming FlowFiles. The properties are used to construct messages of the form: (<PRIORITY>)(VERSION )(TIMESTAMP) (HOSTNAME) (BODY) where version is optional.  The constructed messages are checked against regular expressions for RFC5424 and RFC3164 formatted messages. The timestamp can be an RFC5424 timestamp with a format of \"yyyy-MM-dd'T'HH:mm:ss.SZ\" or \"yyyy-MM-dd'T'HH:mm:ss.S+hh:mm\", or it can be an RFC3164 timestamp with a format of \"MMM d HH:mm:ss\". If a message is constructed that does not form a valid Syslog message according to the above description, then it is routed to the invalid relationship. Valid messages are sent to the Syslog server and successes are routed to the success relationship, failures routed to the failure relationship.","tags":["udp","tcp","syslog","logs","put"]},{"type":"org.apache.nifi.processors.cassandra.QueryCassandra","description":"Execute provided Cassandra Query Language (CQL) select query on a Cassandra 1.x or 2.x cluster. Query result may be converted to Avro or JSON format. Streaming is used so arbitrarily large result sets are supported. This processor can be scheduled to run on a timer, or cron expression, using the standard scheduling methods, or it can be triggered by an incoming FlowFile. If it is triggered by an incoming FlowFile, then attributes of that FlowFile will be available when evaluating the select query. FlowFile attribute 'executecql.row.count' indicates how many rows were selected.","tags":["select","cassandra","cql"]},{"type":"org.apache.nifi.processors.standard.QueryDatabaseTable","description":"Execute provided SQL select query. Query result will be converted to Avro format. Streaming is used so arbitrarily large result sets are supported. This processor can be scheduled to run on a timer, or cron expression, using the standard scheduling methods, or it can be triggered by an incoming FlowFile. If it is triggered by an incoming FlowFile, then attributes of that FlowFile will be available when evaluating the select query. FlowFile attribute 'querydbtable.row.count' indicates how many rows were selected.","tags":["database","select","query","jdbc","sql"]},{"type":"org.apache.nifi.processors.standard.ReplaceText","description":"Updates the content of a FlowFile by evaluating a Regular Expression (regex) against it and replacing the section of the content that matches the Regular Expression with some alternate value.","tags":["Regular Expression","Replace","Regex","Text","Modify","Change","Update"]},{"type":"org.apache.nifi.processors.standard.ReplaceTextWithMapping","description":"Updates the content of a FlowFile by evaluating a Regular Expression against it and replacing the section of the content that matches the Regular Expression with some alternate value provided in a mapping file.","tags":["Regular Expression","Replace","Regex","Text","Modify","Mapping","Change","Update"]},{"type":"org.apache.nifi.processors.image.ResizeImage","description":"Resizes an image to user-specified dimensions. This Processor uses the image codecs registered with the environment that NiFi is running in. By default, this includes JPEG, PNG, BMP, WBMP, and GIF images.","tags":["jpg","image","bmp","gif","png","resize","jpeg","wbmp"]},{"type":"org.apache.nifi.processors.hl7.RouteHL7","description":"Routes incoming HL7 data according to user-defined queries. To add a query, add a new property to the processor. The name of the property will become a new relationship for the processor, and the value is an HL7 Query Language query. If a FlowFile matches the query, a copy of the FlowFile will be routed to the associated relationship.","tags":["HL7","route","healthcare","Health Level 7"]},{"type":"org.apache.nifi.processors.standard.RouteOnAttribute","description":"Routes FlowFiles based on their Attributes using the Attribute Expression Language","tags":["Regular Expression","routing","regexp","regex","Expression Language","Attribute Expression Language","attributes"]},{"type":"org.apache.nifi.processors.standard.RouteOnContent","description":"Applies Regular Expressions to the content of a FlowFile and routes a copy of the FlowFile to each destination whose Regular Expression matches. Regular Expressions are added as User-Defined Properties where the name of the property is the name of the relationship and the value is a Regular Expression to match against the FlowFile content. User-Defined properties do support the Attribute Expression Language, but the results are interpreted as literal values, not Regular Expressions","tags":["regexp","regex","route","regular expression","content"]},{"type":"org.apache.nifi.processors.standard.RouteText","description":"Routes textual data based on a set of user-defined rules. Each line in an incoming FlowFile is compared against the values specified by user-defined Properties. The mechanism by which the text is compared to these user-defined properties is defined by the 'Matching Strategy'. The data is then routed according to these rules, routing each line of the text individually.","tags":["Regular Expression","filter","routing","regexp","regex","Expression Language","csv","delimited","attributes","text","logs"]},{"type":"org.apache.nifi.processors.standard.ScanAttribute","description":"Scans the specified attributes of FlowFiles, checking to see if any of their values are present within the specified dictionary of terms","tags":["lookup","search","scan","attributes"]},{"type":"org.apache.nifi.processors.standard.ScanContent","description":"Scans the content of FlowFiles for terms that are found in a user-supplied dictionary. If a term is matched, the UTF-8 encoded version of the term will be added to the FlowFile using the 'matching.term' attribute","tags":["byte sequence","search","dictionary","find","scan","aho-corasick","content"]},{"type":"org.apache.nifi.processors.standard.SegmentContent","description":"Segments a FlowFile into multiple smaller segments on byte boundaries. Each segment is given the following attributes: fragment.identifier, fragment.index, fragment.count, segment.original.filename; these attributes can then be used by the MergeContent processor in order to reconstitute the original FlowFile","tags":["split","segment"]},{"type":"org.apache.nifi.processors.avro.SplitAvro","description":"Splits a binary encoded Avro datafile into smaller files based on the configured Output Size. The Output Strategy determines if the smaller files will be Avro datafiles, or bare Avro records with metadata in the FlowFile attributes. The output will always be binary encoded.","tags":["split","avro"]},{"type":"org.apache.nifi.processors.standard.SplitContent","description":"Splits incoming FlowFiles by a specified byte sequence","tags":["split","binary","content"]},{"type":"org.apache.nifi.processors.standard.SplitJson","description":"Splits a JSON File into multiple, separate FlowFiles for an array element specified by a JsonPath expression. Each generated FlowFile is comprised of an element of the specified array and transferred to relationship 'split,' with the original file transferred to the 'original' relationship. If the specified JsonPath is not found or does not evaluate to an array element, the original file is routed to 'failure' and no files are generated.","tags":["split","jsonpath","json"]},{"type":"org.apache.nifi.processors.standard.SplitText","description":"Splits a text file into multiple smaller text files on line boundaries, each having up to a configured number of lines.","tags":["split","text"]},{"type":"org.apache.nifi.processors.standard.SplitXml","description":"Splits an XML File into multiple separate FlowFiles, each comprising a child or descendant of the original root element","tags":["split","xml"]},{"type":"org.apache.nifi.spring.SpringContextProcessor","description":"A Processor that supports sending and receiving data from application defined in Spring Application Context via predefined in/out MessageChannels.","tags":["Integration","Message","Get","Spring","Put"]},{"type":"org.apache.nifi.processors.kite.StoreInKiteDataset","description":"Stores Avro records in a Kite dataset","tags":["hive","hdfs","hadoop","kite","hbase","avro","parquet"]},{"type":"org.apache.nifi.processors.standard.TailFile","description":"\"Tails\" a file, ingesting data from the file as it is written to the file. The file is expected to be textual. Data is ingested only when a new line is encountered (carriage return or new-line character or combination). If the file to tail is periodically \"rolled over\", as is generally the case with log files, an optional Rolling Filename Pattern can be used to retrieve data from files that have rolled over, even if the rollover occurred while NiFi was not running (provided that the data still exists upon restart of NiFi). It is generally advisable to set the Run Schedule to a few seconds, rather than running with the default value of 0 secs, as this Processor will consume a lot of resources if scheduled very aggressively. At this time, this Processor does not support ingesting files that have been compressed when 'rolled over'.","tags":["file","log","tail","text","source"]},{"type":"org.apache.nifi.processors.standard.TransformXml","description":"Applies the provided XSLT file to the flowfile XML payload. A new FlowFile is created with transformed content and is routed to the 'success' relationship. If the XSL transform fails, the original FlowFile is routed to the 'failure' relationship","tags":["transform","xml","xslt"]},{"type":"org.apache.nifi.processors.standard.UnpackContent","description":"Unpacks the content of FlowFiles that have been packaged with one of several different Packaging Formats, emitting one to many FlowFiles for each input FlowFile","tags":["zip","flowfile-stream-v3","flowfile-stream","un-merge","tar","archive","Unpack"]},{"type":"org.apache.nifi.processors.attributes.UpdateAttribute","description":"Updates the Attributes for a FlowFile by using the Attribute Expression Language and/or deletes the attributes based on a regular expression","tags":["Attribute Expression Language","update","attributes","modification","delete"]},{"type":"org.apache.nifi.processors.standard.ValidateXml","description":"Validates the contents of FlowFiles against a user-specified XML Schema file","tags":["schema","xml","xsd","validation"]},{"type":"org.apache.nifi.processors.yandex.YandexTranslate","description":"Translates content and attributes from one language to another","tags":["yandex","translation","language","translate"]}]}
